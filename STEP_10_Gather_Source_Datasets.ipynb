{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Gather Source Datasets\n",
    "\n",
    "Purpose: Download and stage required datasets into the project **source** geodatabase.  \n",
    "\n",
    "Current list of resources:\n",
    "\n",
    "- [CRWU_CREAT_Grid_Projections](https://services.arcgis.com/cJ9YHowT8TU7DUyn/ArcGIS/rest/services/CRWU_CREAT_Grid_Projections/FeatureServer/0) from EPA Geoplatform\n",
    "- [CRWU_CREAT_Historic_Climate_Stations](https://services.arcgis.com/cJ9YHowT8TU7DUyn/ArcGIS/rest/services/CRWU_CREAT_Historic_Climate_Stations/FeatureServer/0) from EPA Geoplatform\n",
    "- [COOP_STATIONS_TO_USE](https://github.com/barrc/get_ncei/blob/main/src/coop_stations_to_use.csv) from barrc GitHub\n",
    "- [ISD_STATIONS_TO_USE](https://github.com/barrc/get_ncei/blob/main/src/isd_stations_to_use.csv) from barrc Github\n",
    "- [TEMPORAL_DIST_FILE](https://github.com/barrc/extreme_events/blob/main/temporal_dist_file.txt) from barrc GitHub\n",
    "- [Census States](https://tigerweb.geo.census.gov/arcgis/rest/services/TIGERweb/State_County/MapServer/0) from US Census Tigerweb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing Step 10: Gather Source Datasets\n"
     ]
    }
   ],
   "source": [
    "import arcpy;\n",
    "import os,sys;\n",
    "import json,requests;\n",
    "import datetime,http.client;\n",
    "import zipfile,shutil,csv;\n",
    "import time;\n",
    "\n",
    "print(\"Executing Step 10: Gather Source Datasets\");\n",
    "\n",
    "def swc_resources():\n",
    "    \n",
    "    rez = {};\n",
    "    \n",
    "    # Verify or Create Source filegeodatabase\n",
    "    rez['source'] = os.getcwd() + os.sep + 'source.gdb';\n",
    "\n",
    "    if not arcpy.Exists(rez['source']):\n",
    "        print(\"  creating new source workspace\");\n",
    "        arcpy.CreateFileGDB_management(\n",
    "             os.path.dirname(rez['source'])\n",
    "            ,os.path.basename(rez['source'])\n",
    "        );\n",
    "        \n",
    "    # Verify or Create Working filegeodatabase\n",
    "    rez['working'] = os.getcwd() + os.sep + 'working.gdb';\n",
    "\n",
    "    if not arcpy.Exists(rez['working']):\n",
    "        print(\"  creating new working workspace\");\n",
    "        arcpy.CreateFileGDB_management(\n",
    "             os.path.dirname(rez['working'])\n",
    "            ,os.path.basename(rez['working'])\n",
    "        );\n",
    "        \n",
    "    # Verify or Create Results filegeodatabase\n",
    "    rez['results'] = os.getcwd() + os.sep + 'results.gdb';\n",
    "\n",
    "    if not arcpy.Exists(rez['results']):\n",
    "        print(\"  creating new results workspace\");\n",
    "        arcpy.CreateFileGDB_management(\n",
    "             os.path.dirname(rez['results'])\n",
    "            ,os.path.basename(rez['results'])\n",
    "        );\n",
    "\n",
    "    # Verify or Create qa directory\n",
    "    rez['qa'] = os.getcwd() + os.sep + 'qa';\n",
    "\n",
    "    if not arcpy.Exists(rez['qa']):\n",
    "        print(\"  creating new qa directory\");\n",
    "        os.mkdir(rez['qa']);\n",
    "        \n",
    "    # Verify or Create resources directory\n",
    "    rez['resources'] = os.getcwd() + os.sep + 'resources';\n",
    "\n",
    "    if not arcpy.Exists(rez['resources']):\n",
    "        print(\"  creating new resources directory\");\n",
    "        os.mkdir(rez['resources']);\n",
    "        \n",
    "    # Verify existence of files directory\n",
    "    rez['files'] = os.getcwd() + os.sep + 'files';\n",
    "    \n",
    "    if not arcpy.Exists(rez['resources']):\n",
    "        raise Exception('ERROR: project files directory not found');\n",
    "        \n",
    "    return rez;\n",
    "\n",
    "rez = swc_resources();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape AGS \n",
    "\n",
    "Reusable function to scrape an ArcGIS Online resource into a local file geodatabase.\n",
    "Note some online resources (namely Census) have additional download limits beyond the stated maxRecordCount.\n",
    "Using a smaller forcelimit value will usually work around this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_ags(host,path,fgdb,fc,forcelimit=None):\n",
    "    \n",
    "    if arcpy.Exists(fgdb + os.sep + fc):\n",
    "        arcpy.Delete_management(fgdb + os.sep + fc);\n",
    "        \n",
    "    headers = {\"Content-type\": \"application/x-www-form-urlencoded\", \"Accept\": \"text/plain\"};\n",
    "    conn = http.client.HTTPSConnection(host);\n",
    "    conn.request(\"POST\",path,\"f=json\",headers);\n",
    "    response = conn.getresponse();\n",
    "    data = response.read();\n",
    "    json_data = json.loads(data);\n",
    "    if not 'currentVersion' in json_data:\n",
    "        raise ValueError(\"Error, unable to query https://\" + host + path);\n",
    "    extraction_amount = json_data['maxRecordCount'];\n",
    "    if forcelimit is not None and forcelimit < extraction_amount:\n",
    "        extraction_amount = forcelimit;\n",
    "    where = \"1=1\";\n",
    "    params = \"where={}&returnIdsOnly=true&returnGeometry=false&f=json\".format(where);\n",
    "    conn = http.client.HTTPSConnection(host);\n",
    "    conn.request(\"POST\",path + \"/query\",params,headers);\n",
    "    response = conn.getresponse();\n",
    "    data = response.read();\n",
    "    json_data = json.loads(data);\n",
    "    ary_oid   = sorted(json_data['objectIds']);\n",
    "    oid_name  = json_data['objectIdFieldName'];\n",
    "    oid_count = len(ary_oid);\n",
    "    \n",
    "    initial_hit = True;\n",
    "    counter = 0;\n",
    "    while counter <= oid_count - 1:\n",
    "        if counter + extraction_amount > oid_count - 1:\n",
    "            int_max = oid_count - 1;\n",
    "        else:\n",
    "            int_max = counter + extraction_amount - 1;\n",
    "        where = oid_name + ' >= ' + str(ary_oid[counter]) + ' AND ' + oid_name + ' <= ' + str(ary_oid[int_max]);\n",
    "        print(\"  pulling records where \" + where);\n",
    "        fields = \"*\";\n",
    "        params = \"where={}&outFields={}&returnGeometry=true&outSR=4269&f=json\".format(where, fields);\n",
    "        conn = http.client.HTTPSConnection(host);\n",
    "        conn.request(\"POST\",path + \"/query\",params,headers);\n",
    "        response = conn.getresponse();\n",
    "        data = response.read(); \n",
    "        json_data = json.loads(data);\n",
    "        ef = arcpy.AsShape(json_data,True);\n",
    "        if initial_hit:\n",
    "            arcpy.management.CopyFeatures(ef,fgdb + os.sep + fc)\n",
    "            initial_hit = False;\n",
    "        else:\n",
    "            arcpy.Append_management(ef,fgdb + os.sep + fc,\"NO_TEST\");\n",
    "        counter += extraction_amount;\n",
    "        \n",
    "    conn.close(); \n",
    "    del conn;\n",
    "    print(\"  Scrape complete.\");\n",
    "    return True;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.010: Download CRWU_CREAT_Grid_Projections from EPA Geoplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  pulling records where OBJECTID >= 1 AND OBJECTID <= 2000\n",
      "  pulling records where OBJECTID >= 2001 AND OBJECTID <= 4000\n",
      "  pulling records where OBJECTID >= 4001 AND OBJECTID <= 6000\n",
      "  pulling records where OBJECTID >= 6001 AND OBJECTID <= 8000\n",
      "  pulling records where OBJECTID >= 8001 AND OBJECTID <= 10000\n",
      "  pulling records where OBJECTID >= 10001 AND OBJECTID <= 12000\n",
      "  pulling records where OBJECTID >= 12001 AND OBJECTID <= 14000\n",
      "  pulling records where OBJECTID >= 14001 AND OBJECTID <= 16000\n",
      "  pulling records where OBJECTID >= 16001 AND OBJECTID <= 18000\n",
      "  pulling records where OBJECTID >= 18001 AND OBJECTID <= 20000\n",
      "  pulling records where OBJECTID >= 20001 AND OBJECTID <= 22000\n",
      "  pulling records where OBJECTID >= 22001 AND OBJECTID <= 24000\n",
      "  pulling records where OBJECTID >= 24001 AND OBJECTID <= 24743\n",
      "  Scrape complete.\n",
      "Wall time: 4min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "host = \"services.arcgis.com\";\n",
    "path = \"/cJ9YHowT8TU7DUyn/ArcGIS/rest/services/CRWU_CREAT_Grid_Projections/FeatureServer/0\";\n",
    "fc   = \"CRWU_CREAT_Grid_Projections\";\n",
    "\n",
    "if arcpy.Exists(rez['source'] + os.sep + fc):\n",
    "    arcpy.Delete_management(rez['source'] + os.sep + fc);\n",
    "\n",
    "z = scrape_ags(host,path,rez['source'],fc);\n",
    "\n",
    "z = arcpy.management.AddIndex(\n",
    "     in_table   = rez['source'] + os.sep + fc\n",
    "    ,fields     = 'CREAT_ID'\n",
    "    ,index_name = 'CREAT_ID_IDX'\n",
    ");\n",
    "\n",
    "z = arcpy.management.AddIndex(\n",
    "     in_table   = rez['source'] + os.sep + fc\n",
    "    ,fields     = 'GRIDCODE'\n",
    "    ,index_name = 'GRIDCODE_IDX'\n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.020: Download CRWU_CREAT_Historic_Climate_Stations from EPA Geoplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  pulling records where OBJECTID >= 1 AND OBJECTID <= 2000\n",
      "  pulling records where OBJECTID >= 2001 AND OBJECTID <= 4000\n",
      "  pulling records where OBJECTID >= 4001 AND OBJECTID <= 6000\n",
      "  pulling records where OBJECTID >= 6001 AND OBJECTID <= 8000\n",
      "  pulling records where OBJECTID >= 8001 AND OBJECTID <= 10000\n",
      "  pulling records where OBJECTID >= 10001 AND OBJECTID <= 11165\n",
      "  Scrape complete.\n",
      "Wall time: 11.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "host = \"services.arcgis.com\";\n",
    "path = \"/cJ9YHowT8TU7DUyn/ArcGIS/rest/services/CRWU_CREAT_Historic_Climate_Stations/FeatureServer/0\";\n",
    "fc   = \"CRWU_CREAT_Historic_Climate_Stations\";\n",
    "\n",
    "if arcpy.Exists(rez['source'] + os.sep + fc):\n",
    "    arcpy.Delete_management(rez['source'] + os.sep + fc);\n",
    "\n",
    "z = scrape_ags(host,path,rez['source'],fc);\n",
    "\n",
    "z = arcpy.management.AddIndex(\n",
    "     in_table   = rez['source'] + os.sep + fc\n",
    "    ,fields     = 'CLIMATE_STATION_PK_ID'\n",
    "    ,index_name = 'CLIMATE_STATION_PK_ID_IDX'\n",
    ");\n",
    "\n",
    "z = arcpy.management.AddIndex(\n",
    "     in_table   = rez['source'] + os.sep + fc\n",
    "    ,fields     = 'NOAA_STATION_ID'\n",
    "    ,index_name = 'NOAA_STATION_ID_IDX'\n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tab Downloaders\n",
    "\n",
    "Reusable functions to download and import csv or tab delimited text into local filegeodatabase \n",
    "using optional custom fieldmappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadtab(url,filename):\n",
    "    if arcpy.Exists(filename):\n",
    "        arcpy.Delete_management(filename);\n",
    "    print(\"  downloading file\");\n",
    "    with open(filename,'wb') as f,requests.get(url,stream=True) as r:\n",
    "        for line in r.iter_lines():\n",
    "            f.write(line + '\\n'.encode());\n",
    "    return True;\n",
    "    \n",
    "def tab2fc(filename,fgdb,fc,longname,latname,field_mapping=None):\n",
    "    \n",
    "    if arcpy.Exists('memory' + os.sep + 'tempTable'):\n",
    "        arcpy.Delete_management('memory' + os.sep + 'tempTable');\n",
    "  \n",
    "    print(\"  loading to table\");\n",
    "    arcpy.TableToTable_conversion(\n",
    "         in_rows       = filename\n",
    "        ,out_path      = 'memory'\n",
    "        ,out_name      = 'tempTable'\n",
    "        ,field_mapping = field_mapping\n",
    "    );\n",
    "    \n",
    "    if arcpy.Exists(fgdb + os.sep + fc):\n",
    "        arcpy.Delete_management(fgdb + os.sep + fc);\n",
    "        \n",
    "    print(\"  converting to NAD83 points\");\n",
    "    arcpy.management.XYTableToPoint(\n",
    "         in_table          = 'memory' + os.sep + 'tempTable'\n",
    "        ,out_feature_class = fgdb + os.sep + fc\n",
    "        ,x_field           = longname\n",
    "        ,y_field           = latname\n",
    "        ,coordinate_system = arcpy.SpatialReference(4269)\n",
    "    );\n",
    "    \n",
    "    arcpy.Delete_management('memory' + os.sep + 'tempTable');\n",
    "    return True;\n",
    "\n",
    "def tab2tab(filename,fgdb,fc,field_mapping=None):\n",
    "    \n",
    "    if arcpy.Exists(fgdb + os.sep + fc):\n",
    "        arcpy.Delete_management(fgdb + os.sep + fc);\n",
    "    \n",
    "    print(\"  loading to table\");\n",
    "    arcpy.TableToTable_conversion(\n",
    "         in_rows       = filename\n",
    "        ,out_path      = fgdb\n",
    "        ,out_name      = fc\n",
    "        ,field_mapping = field_mapping\n",
    "    );\n",
    "        \n",
    "    return True;\n",
    "\n",
    "def fmtext(infc,fieldname,fieldlength):\n",
    "    fm = arcpy.FieldMap();\n",
    "    fm.addInputField(infc,fieldname);\n",
    "    nf = fm.outputField;\n",
    "    nf.type = 'Text';\n",
    "    nf.length = fieldlength;\n",
    "    fm.outputField = nf;\n",
    "    return fm;\n",
    "\n",
    "def fmint(infc,fieldname):\n",
    "    fm = arcpy.FieldMap();\n",
    "    fm.addInputField(infc,fieldname);\n",
    "    nf = fm.outputField;\n",
    "    nf.type = 'Integer';\n",
    "    fm.outputField = nf;\n",
    "    return fm;\n",
    "\n",
    "def fmdouble(infc,fieldname):\n",
    "    fm = arcpy.FieldMap();\n",
    "    fm.addInputField(infc,fieldname);\n",
    "    nf = fm.outputField;\n",
    "    nf.type = 'Double';\n",
    "    fm.outputField = nf;\n",
    "    return fm;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.030: Download COOP_STATIONS_TO_USE dataset from barrc GitHub repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  downloading file\n",
      "  loading to table\n",
      "  converting to NAD83 points\n",
      "  check for missing stationIDs\n",
      "  add quotes to start and end fields\n",
      "  calculating year count\n",
      "  adding indexes\n",
      "Wall time: 12.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/barrc/get_ncei/master/src/coop_stations_to_use.csv\"\n",
    "fc  = 'COOP_STATIONS_TO_USE';\n",
    "\n",
    "tmptab = rez['qa'] + os.sep + 'coop_stations_to_use.csv';\n",
    "z = downloadtab(url,tmptab);\n",
    "\n",
    "fmscoop = arcpy.FieldMappings();\n",
    "fmscoop.addFieldMap(fmtext  (tmptab,'station_id',255));\n",
    "fmscoop.addFieldMap(fmtext  (tmptab,'station_name',255));\n",
    "fmscoop.addFieldMap(fmtext  (tmptab,'state',255));\n",
    "fmscoop.addFieldMap(fmtext  (tmptab,'start_date',255));\n",
    "fmscoop.addFieldMap(fmtext  (tmptab,'end_date',255));\n",
    "fmscoop.addFieldMap(fmdouble(tmptab,'latitude'));\n",
    "fmscoop.addFieldMap(fmdouble(tmptab,'longitude'));\n",
    "fmscoop.addFieldMap(fmtext  (tmptab,'in_basins',255));\n",
    "fmscoop.addFieldMap(fmtext  (tmptab,'break_with_basins',255));\n",
    "fmscoop.addFieldMap(fmtext  (tmptab,'network',255));\n",
    "fmscoop.addFieldMap(fmtext  (tmptab,'start_date_to_use',255));\n",
    "fmscoop.addFieldMap(fmtext  (tmptab,'end_date_to_use',255));\n",
    "\n",
    "z = tab2fc(tmptab,rez['source'],fc,'longitude','latitude',fmscoop);\n",
    "\n",
    "print(\"  check for missing stationIDs\");\n",
    "z = arcpy.management.CalculateField(\n",
    "     in_table        = rez['source'] + os.sep + fc\n",
    "    ,field           = 'station_id'\n",
    "    ,expression      = \"injectID(!station_id!,!OBJECTID!)\"\n",
    "    ,expression_type = 'PYTHON3'\n",
    "    ,code_block      = \"\"\"\n",
    "def injectID(pin,oid):\n",
    "    if pin is None:\n",
    "        return 'COOP' + str(oid);\n",
    "    else:\n",
    "        return pin;\n",
    "\"\"\");\n",
    "\n",
    "print(\"  add quotes to start and end fields\");\n",
    "cb_cleanDate = \"\"\"\n",
    "def cleanDate(pin):\n",
    "    (mm,dd,yyyy) = pin.split('/');\n",
    "    if mm in ['1','2','3','4','5','6','7','8','9']:\n",
    "       mm = '0' + mm;\n",
    "    if dd in ['1','2','3','4','5','6','7','8','9']:\n",
    "       dd = '0' + dd;\n",
    "    return \"'\" + yyyy + \"/\" + mm + \"/\" + dd + \"'\";\n",
    "    \n",
    "\"\"\";\n",
    "\n",
    "z = arcpy.management.AddField(\n",
    "     in_table     = rez['source'] + os.sep + fc\n",
    "    ,field_name   = 'start_date_clean'\n",
    "    ,field_type   = 'Text'\n",
    "    ,field_length = 255\n",
    "    ,field_alias  = 'start_date_clean'\n",
    ");\n",
    "\n",
    "z = arcpy.management.CalculateField(\n",
    "     in_table        = rez['source'] + os.sep + fc\n",
    "    ,field           = 'start_date_clean'\n",
    "    ,expression      = \"cleanDate(!start_date!)\"\n",
    "    ,expression_type = 'PYTHON3'\n",
    "    ,code_block      = cb_cleanDate\n",
    ");\n",
    "\n",
    "z = arcpy.management.AddField(\n",
    "     in_table     = rez['source'] + os.sep + fc\n",
    "    ,field_name   = 'end_date_clean'\n",
    "    ,field_type   = 'Text'\n",
    "    ,field_length = 255\n",
    "    ,field_alias  = 'end_date_clean'\n",
    ");\n",
    "\n",
    "z = arcpy.management.CalculateField(\n",
    "     in_table        = rez['source'] + os.sep + fc\n",
    "    ,field           = 'end_date_clean'\n",
    "    ,expression      = \"cleanDate(!end_date!)\"\n",
    "    ,expression_type = 'PYTHON3'\n",
    "    ,code_block      = cb_cleanDate\n",
    ");\n",
    "\n",
    "z = arcpy.management.AddField(\n",
    "     in_table     = rez['source'] + os.sep + fc\n",
    "    ,field_name   = 'start_date_to_use_clean'\n",
    "    ,field_type   = 'Text'\n",
    "    ,field_length = 255\n",
    "    ,field_alias  = 'start_date_to_use_clean'\n",
    ");\n",
    "\n",
    "z = arcpy.management.CalculateField(\n",
    "     in_table        = rez['source'] + os.sep + fc\n",
    "    ,field           = 'start_date_to_use_clean'\n",
    "    ,expression      = \"cleanDate(!start_date_to_use!)\"\n",
    "    ,expression_type = 'PYTHON3'\n",
    "    ,code_block      = cb_cleanDate\n",
    ");\n",
    "\n",
    "z = arcpy.management.AddField(\n",
    "     in_table     = rez['source'] + os.sep + fc\n",
    "    ,field_name   = 'end_date_to_use_clean'\n",
    "    ,field_type   = 'Text'\n",
    "    ,field_length = 255\n",
    "    ,field_alias  = 'end_date_to_use_clean'\n",
    ");\n",
    "\n",
    "z = arcpy.management.CalculateField(\n",
    "     in_table        = rez['source'] + os.sep + fc\n",
    "    ,field           = 'end_date_to_use_clean'\n",
    "    ,expression      = \"cleanDate(!end_date_to_use!)\"\n",
    "    ,expression_type = 'PYTHON3'\n",
    "    ,code_block      = cb_cleanDate\n",
    ");\n",
    "\n",
    "print(\"  calculating year count\");\n",
    "cb_yearCount = \"\"\"\n",
    "import datetime;\n",
    "def yearCount(pstart,pend):\n",
    "    d1 = datetime.datetime.strptime(pstart,\"%m/%d/%Y\");\n",
    "    d2 = datetime.datetime.strptime(pend  ,\"%m/%d/%Y\");\n",
    "    yr = round((d2 - d1).days / 365);\n",
    "    return yr + 0.0;\n",
    "    \n",
    "\"\"\";\n",
    "\n",
    "z = arcpy.management.AddField(\n",
    "     in_table     = rez['source'] + os.sep + fc\n",
    "    ,field_name   = 'year_count'\n",
    "    ,field_type   = 'Double'\n",
    "    ,field_alias  = 'year_count'\n",
    ");\n",
    "\n",
    "z = arcpy.management.CalculateField(\n",
    "     in_table        = rez['source'] + os.sep + fc\n",
    "    ,field           = 'year_count'\n",
    "    ,expression      = 'yearCount(!start_date_to_use!,!end_date_to_use!)'\n",
    "    ,expression_type = 'PYTHON3'\n",
    "    ,code_block      = cb_yearCount\n",
    ");\n",
    "\n",
    "print(\"  adding indexes\");\n",
    "z = arcpy.management.AddIndex(\n",
    "     in_table      = rez['source']+ os.sep + fc\n",
    "    ,fields        = 'station_id'\n",
    "    ,index_name    = 'station_id_IDX'\n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.040: Download ISD_STATIONS_TO_USE dataset from barrc GitHub repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  downloading file\n",
      "  loading to table\n",
      "  converting to NAD83 points\n",
      "  check for missing stationIDs\n",
      "  add quotes to start and end fields\n",
      "  calculating year count\n",
      "  adding indexes\n",
      "Wall time: 9.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/barrc/get_ncei/master/src/isd_stations_to_use.csv\"\n",
    "fc  = 'ISD_STATIONS_TO_USE';\n",
    "\n",
    "tmptab = rez['qa'] + os.sep + 'isd_stations_to_use.csv';\n",
    "z = downloadtab(url,tmptab);\n",
    "\n",
    "fmsisd = arcpy.FieldMappings();\n",
    "fmsisd.addFieldMap(fmtext  (tmptab,'station_id',255));\n",
    "fmsisd.addFieldMap(fmtext  (tmptab,'station_name',255));\n",
    "fmsisd.addFieldMap(fmtext  (tmptab,'state',255));\n",
    "fmsisd.addFieldMap(fmtext  (tmptab,'start_date',255));\n",
    "fmsisd.addFieldMap(fmtext  (tmptab,'end_date',255));\n",
    "fmsisd.addFieldMap(fmdouble(tmptab,'latitude'));\n",
    "fmsisd.addFieldMap(fmdouble(tmptab,'longitude'));\n",
    "fmsisd.addFieldMap(fmtext  (tmptab,'in_basins',255));\n",
    "fmsisd.addFieldMap(fmtext  (tmptab,'break_with_basins',255));\n",
    "fmsisd.addFieldMap(fmtext  (tmptab,'network',255));\n",
    "\n",
    "z = tab2fc(tmptab,rez['source'],fc,'longitude','latitude',fmsisd);\n",
    "\n",
    "print(\"  check for missing stationIDs\");\n",
    "z = arcpy.management.CalculateField(\n",
    "     in_table        = rez['source'] + os.sep + fc\n",
    "    ,field           = 'station_id'\n",
    "    ,expression      = \"injectID(!station_id!,!OBJECTID!)\"\n",
    "    ,expression_type = 'PYTHON3'\n",
    "    ,code_block      = \"\"\"\n",
    "def injectID(pin,oid):\n",
    "    if pin is None:\n",
    "        return 'ISD' + str(oid);\n",
    "    else:\n",
    "        return pin;\n",
    "\"\"\");\n",
    "\n",
    "print(\"  add quotes to start and end fields\");\n",
    "z = arcpy.management.AddField(\n",
    "     in_table     = rez['source'] + os.sep + fc\n",
    "    ,field_name   = 'start_date_clean'\n",
    "    ,field_type   = 'Text'\n",
    "    ,field_length = 255\n",
    "    ,field_alias  = 'start_date_clean'\n",
    ");\n",
    "\n",
    "z = arcpy.management.CalculateField(\n",
    "     in_table        = rez['source'] + os.sep + fc\n",
    "    ,field           = 'start_date_clean'\n",
    "    ,expression      = \"cleanDate(!start_date!)\"\n",
    "    ,expression_type = 'PYTHON3'\n",
    "    ,code_block      = cb_cleanDate\n",
    ");\n",
    "\n",
    "z = arcpy.management.AddField(\n",
    "     in_table     = rez['source'] + os.sep + fc\n",
    "    ,field_name   = 'end_date_clean'\n",
    "    ,field_type   = 'Text'\n",
    "    ,field_length = 255\n",
    "    ,field_alias  = 'end_date_clean'\n",
    ");\n",
    "\n",
    "z = arcpy.management.CalculateField(\n",
    "     in_table        = rez['source'] + os.sep + fc\n",
    "    ,field           = 'end_date_clean'\n",
    "    ,expression      = \"cleanDate(!end_date!)\"\n",
    "    ,expression_type = 'PYTHON3'\n",
    "    ,code_block      = cb_cleanDate\n",
    ");\n",
    "\n",
    "print(\"  calculating year count\");\n",
    "z = arcpy.management.AddField(\n",
    "     in_table     = rez['source'] + os.sep + fc\n",
    "    ,field_name   = 'year_count'\n",
    "    ,field_type   = 'Double'\n",
    "    ,field_alias  = 'year_count'\n",
    ");\n",
    "\n",
    "z = arcpy.management.CalculateField(\n",
    "     in_table        = rez['source'] + os.sep + fc\n",
    "    ,field           = 'year_count'\n",
    "    ,expression      = 'yearCount(!start_date!,!end_date!)'\n",
    "    ,expression_type = 'PYTHON3'\n",
    "    ,code_block      = cb_yearCount\n",
    ");\n",
    "\n",
    "print(\"  adding indexes\");\n",
    "z = arcpy.management.AddIndex(\n",
    "     in_table   = rez['source'] + os.sep + fc\n",
    "    ,fields     = 'station_id'\n",
    "    ,index_name = 'station_id_IDX'\n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.050: Download TEMPORAL_DIST_FILE dataset from barrc GitHub repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  downloading file\n",
      "  loading to table\n",
      "  adding indexes\n",
      "Wall time: 2.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/barrc/extreme_events/main/temporal_dist_file.txt\"\n",
    "fc  = 'TEMPORAL_DIST_FILE';\n",
    "\n",
    "tmptab = rez['qa'] + os.sep + 'temporal_dist_file.tab';\n",
    "z = downloadtab(url,tmptab);\n",
    "\n",
    "fms = arcpy.FieldMappings();\n",
    "fms.addFieldMap(fmdouble(tmptab,'Time'));\n",
    "fms.addFieldMap(fmdouble(tmptab,'CA_1'));\n",
    "fms.addFieldMap(fmdouble(tmptab,'CA_2'));\n",
    "fms.addFieldMap(fmdouble(tmptab,'CA_3'));\n",
    "fms.addFieldMap(fmdouble(tmptab,'CA_4'));\n",
    "fms.addFieldMap(fmdouble(tmptab,'CA_5'));\n",
    "fms.addFieldMap(fmdouble(tmptab,'CA_6'));\n",
    "fms.addFieldMap(fmdouble(tmptab,'MSE_1'));\n",
    "fms.addFieldMap(fmdouble(tmptab,'MSE_2'));\n",
    "fms.addFieldMap(fmdouble(tmptab,'MSE_3'));\n",
    "fms.addFieldMap(fmdouble(tmptab,'MSE_4'));\n",
    "fms.addFieldMap(fmdouble(tmptab,'MSE_5'));\n",
    "fms.addFieldMap(fmdouble(tmptab,'MSE_6'));\n",
    "fms.addFieldMap(fmdouble(tmptab,'NOAA_A'));\n",
    "fms.addFieldMap(fmdouble(tmptab,'NOAA_B'));\n",
    "fms.addFieldMap(fmdouble(tmptab,'NOAA_C'));\n",
    "fms.addFieldMap(fmdouble(tmptab,'NOAA_D'));\n",
    "fms.addFieldMap(fmdouble(tmptab,'NRCC_A'));\n",
    "fms.addFieldMap(fmdouble(tmptab,'NRCC_B'));\n",
    "fms.addFieldMap(fmdouble(tmptab,'NRCC_C'));\n",
    "fms.addFieldMap(fmdouble(tmptab,'NRCC_D'));\n",
    "fms.addFieldMap(fmdouble(tmptab,'NV_N'));\n",
    "fms.addFieldMap(fmdouble(tmptab,'NV_S'));\n",
    "fms.addFieldMap(fmdouble(tmptab,'NV_W'));\n",
    "fms.addFieldMap(fmdouble(tmptab,'SCS_I'));\n",
    "fms.addFieldMap(fmdouble(tmptab,'SCS_IA'));\n",
    "fms.addFieldMap(fmdouble(tmptab,'SCS_II'));\n",
    "fms.addFieldMap(fmdouble(tmptab,'SCS_III'));\n",
    "\n",
    "z = tab2tab(tmptab,rez['source'],fc,fms);\n",
    "\n",
    "print(\"  adding indexes\");\n",
    "z = arcpy.management.AddIndex(\n",
    "     in_table   = rez['source'] + os.sep + fc\n",
    "    ,fields     = 'time'\n",
    "    ,index_name = 'time_IDX'\n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.060: Download US Census Tigerweb 2020 State Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  pulling records where OBJECTID >= 1 AND OBJECTID <= 5\n",
      "  pulling records where OBJECTID >= 6 AND OBJECTID <= 10\n",
      "  pulling records where OBJECTID >= 11 AND OBJECTID <= 15\n",
      "  pulling records where OBJECTID >= 16 AND OBJECTID <= 20\n",
      "  pulling records where OBJECTID >= 21 AND OBJECTID <= 25\n",
      "  pulling records where OBJECTID >= 26 AND OBJECTID <= 30\n",
      "  pulling records where OBJECTID >= 31 AND OBJECTID <= 35\n",
      "  pulling records where OBJECTID >= 36 AND OBJECTID <= 40\n",
      "  pulling records where OBJECTID >= 41 AND OBJECTID <= 45\n",
      "  pulling records where OBJECTID >= 46 AND OBJECTID <= 50\n",
      "  pulling records where OBJECTID >= 51 AND OBJECTID <= 55\n",
      "  pulling records where OBJECTID >= 56 AND OBJECTID <= 56\n",
      "  Scrape complete.\n",
      "  adding indexes\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Note tigerweb will timeout if all state-equivalent records are requested in one go.\n",
    "# Setting the forcelimit value to 5 records at once works around the problem.\n",
    "\n",
    "host = \"tigerweb.geo.census.gov\";\n",
    "path = \"/arcgis/rest/services/TIGERweb/State_County/MapServer/0\";\n",
    "fc   = \"census_states\";\n",
    "\n",
    "if arcpy.Exists(rez['source'] + os.sep + fc):\n",
    "    arcpy.Delete_management(rez['source'] + os.sep + fc);\n",
    "\n",
    "z = scrape_ags(host,path,rez['source'],fc,5);\n",
    "\n",
    "print(\"  adding indexes\");\n",
    "z = arcpy.management.AddIndex(\n",
    "     in_table   = rez['source'] + os.sep + fc\n",
    "    ,fields     = 'GEOID'\n",
    "    ,index_name = 'GEOID_IDX'\n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.070: Review and QA\n",
    "\n",
    "QA Products:\n",
    "\n",
    "1. flat files saved to **qa** folder\n",
    "2. counts saved to **step10qa.txt**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Grid Projections : 24743\n",
      "  Historic Stations: 11165\n",
      "  COOP Stations    : 1851\n",
      "  ISD Stations     : 3293\n",
      "  Tigerweb States  : 56\n",
      " \n",
      "Wall time: 1.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "grid = rez['source']  + os.sep + 'CRWU_CREAT_Grid_Projections';\n",
    "grid_cnt = arcpy.GetCount_management(grid)[0];\n",
    "hist = rez['source']  + os.sep + 'CRWU_CREAT_Historic_Climate_Stations';\n",
    "hist_cnt = arcpy.GetCount_management(hist)[0];\n",
    "coop = rez['source']  + os.sep + 'COOP_STATIONS_TO_USE';\n",
    "coop_cnt = arcpy.GetCount_management(coop)[0];\n",
    "isd  = rez['source']  + os.sep + 'ISD_STATIONS_TO_USE';\n",
    "isd_cnt = arcpy.GetCount_management(isd)[0];\n",
    "tdf  = rez['source']  + os.sep + 'TEMPORAL_DIST_FILE';\n",
    "tdf_cnt = arcpy.GetCount_management(tdf)[0];\n",
    "states  = rez['source'] + os.sep + 'census_states';\n",
    "states_cnt = arcpy.GetCount_management(states)[0];\n",
    "\n",
    "print(\"  Grid Projections : \" + str(grid_cnt));\n",
    "print(\"  Historic Stations: \" + str(hist_cnt));\n",
    "print(\"  COOP Stations    : \" + str(coop_cnt));\n",
    "print(\"  ISD Stations     : \" + str(isd_cnt));\n",
    "print(\"  Tigerweb States  : \" + str(states_cnt));\n",
    "print(\" \");\n",
    "\n",
    "nw = datetime.datetime.now();\n",
    "with open(rez['qa'] + os.sep + 'step10qa.txt',\"w\") as out:\n",
    "    out.write(\"Step 10 QA Review\\n\");\n",
    "    out.write(datetime.datetime.now().isoformat() + \"\\n\");\n",
    "    out.write(\"Grid Projections Loaded: \" + str(grid_cnt) + \"\\n\");\n",
    "    out.write(\"Historic Stations Loaded: \" + str(hist_cnt) + \"\\n\");\n",
    "    out.write(\"COOP Stations Loaded: \" + str(coop_cnt) + \"\\n\");\n",
    "    out.write(\"ISD Stations Loaded: \" + str(isd_cnt) + \"\\n\");\n",
    "    out.write(\"Temp Dist File Records Loaded: \" + str(tdf_cnt) + \"\\n\");\n",
    "    out.write(\"Tigerweb States Loaded: \" + str(states_cnt) + \"\\n\");\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
