{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRISM Temperature Download\n",
    "\n",
    "Goals:\n",
    "\n",
    "1. For each SWC station, pull PRISM results for the given coordinate location between 1990 and 2020.\n",
    "2. Aggregate results together into CSV file.\n",
    "\n",
    "Notes:\n",
    "    \n",
    "* The logic should allow for downloading a year's worth of data for 500 stations.  However, some kind of internal limiter at the PRISM site detects this bulk attempt and limits the results to the first five locations only.  I am not able to circumvent this and thus the current logic just does five at a time.  This requires about 10 hours to complete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Initial SWC Station Universe Count: 5144\n"
     ]
    }
   ],
   "source": [
    "import arcpy;\n",
    "import os,sys;\n",
    "import requests,csv;\n",
    "import datetime;\n",
    "from dateutil.rrule import rrule,DAILY,YEARLY;\n",
    "from time import sleep;\n",
    "\n",
    "start_date  = datetime.date(1990,1,1);\n",
    "end_date    = datetime.date(2020,12,31);\n",
    "\n",
    "results_fgdb = os.getcwd() + os.sep + '..'+ os.sep + 'results.gdb';\n",
    "target_dir   = os.getcwd() + os.sep + 'prism';\n",
    "\n",
    "if not os.path.exists(target_dir):\n",
    "    os.mkdir(target_dir);\n",
    "\n",
    "stations = results_fgdb  + os.sep + 'SWC_Station_Universe';\n",
    "stations_cnt = arcpy.GetCount_management(stations)[0];\n",
    "print(\"  Initial SWC Station Universe Count: \" + str(stations_cnt));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  creating station tickers as a hash.\n",
      "Wall time: 230 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "stations   = results_fgdb + os.sep + 'SWC_Station_Universe';\n",
    "tickers = {};\n",
    "\n",
    "flds = [\n",
    "     'StationId'\n",
    "    ,'SHAPE@'\n",
    "];\n",
    "\n",
    "ticker_cnt  = 1;\n",
    "ticker_size = 5;\n",
    "ticker_max  = None;\n",
    "\n",
    "print(\"  creating station tickers as a hash.\");\n",
    "with arcpy.da.SearchCursor(\n",
    "     in_table     = stations\n",
    "    ,field_names  = flds\n",
    "    ,where_clause = \"CONUSFlag = 'Y'\"\n",
    ") as incur:\n",
    "\n",
    "    ticker = 1;\n",
    "    outfile = None;\n",
    "    for row in incur:\n",
    "    \n",
    "        point = row[1].firstPoint;\n",
    "        lat  = str(round(point.Y,8));\n",
    "        lon  = str(round(point.X,8));\n",
    "        name = row[0];\n",
    "        \n",
    "        if ticker == 1:\n",
    "            tickers[ticker_cnt] = {\n",
    "                 \"lats\" : lat\n",
    "                ,\"lons\" : lon\n",
    "                ,\"names\": name\n",
    "            };\n",
    "            \n",
    "        else:\n",
    "            tickers[ticker_cnt]['lats']  = tickers[ticker_cnt]['lats']  + '|' + lat;\n",
    "            tickers[ticker_cnt]['lons']  = tickers[ticker_cnt]['lons']  + '|' + lon;\n",
    "            tickers[ticker_cnt]['names'] = tickers[ticker_cnt]['names'] + '|' + name;\n",
    "\n",
    "        ticker += 1;\n",
    "        if ticker == ticker_size + 1:\n",
    "            ticker_cnt += 1;\n",
    "            ticker = 1;\n",
    "            \n",
    "            if ticker_max is not None and ticker_cnt > ticker_max:\n",
    "                break;\n",
    "            \n",
    "#tickers\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleeper = 1;\n",
    "url = 'https://prism.oregonstate.edu/explorer/dataexplorer/rpc.php';\n",
    "pickup = 'https://prism.oregonstate.edu/explorer/tmp/'\n",
    "# These headers are probably utterly irrelevant to the prism servers\n",
    "headers = {\n",
    "     'Host'            : 'prism.oregonstate.edu' \n",
    "    ,'Connection'      : 'keep-alive'\n",
    "    ,'Content-Length'  : '7434'\n",
    "    ,'Pragma'          : 'no-cache'\n",
    "    ,'Cache-Control'   : 'no-cache'\n",
    "    ,'sec-ch-ua'       : '\"Chromium\";v=\"92\", \" Not A;Brand\";v=\"99\", \"Google Chrome\";v=\"92\"'\n",
    "    ,'sec-ch-ua-mobile': '?0'\n",
    "    ,'Sec-Fetch-Dest'  : 'empty'\n",
    "    ,'Sec-Fetch-Mode'  : 'cors'\n",
    "    ,'Sec-Fetch-Site'  : 'same-origin'\n",
    "    ,'Accept'          : 'application/json, text/javascript, */*; q=0.01'\n",
    "    ,'Content-Type'    : 'application/x-www-form-urlencoded; charset=UTF-8'\n",
    "    ,'Accept-Language' : 'en-us'\n",
    "    ,'Accept-Encoding' : 'gzip, deflate'\n",
    "    ,'Origin'          : 'https://www.prism.oregonstate.edu'\n",
    "    ,'Referer'         : 'https://www.prism.oregonstate.edu/explorer/bulk.php'\n",
    "    ,'User-Agent'      : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.131 Safari/537.36'\n",
    "    ,'X-Requested-With': 'XMLHttpRequest'  \n",
    "    ,'DNT'             : '1'\n",
    "}\n",
    "\n",
    "for k,v in tickers.items():\n",
    "    \n",
    "    # Use this logic bypass if you need to restart following a network failure\n",
    "    if k >= 0:\n",
    "        \n",
    "        print(\" Processing \" + str(k));\n",
    "\n",
    "        target_file = target_dir + os.sep + 'batch' + str(k) + '.csv';\n",
    "        if os.path.exists(target_file):\n",
    "            os.remove(target_file);\n",
    "\n",
    "        for yr in rrule(\n",
    "             freq    = YEARLY\n",
    "            ,dtstart = start_date\n",
    "            ,until   = end_date\n",
    "        ):\n",
    "            payload_outer = {\n",
    "                 'call'     : 'pp/daily_timeseries_mp'\n",
    "                ,'proc'     : 'gridserv'\n",
    "                ,'lons'     : v['lons']\n",
    "                ,'lats'     : v['lats']\n",
    "                ,'names'    : v['names']\n",
    "                ,'spares'   : '4km'\n",
    "                ,'interp'   : 0\n",
    "                ,'stats'    : 'tmin tmax'\n",
    "                ,'units'    : 'eng'\n",
    "                ,'range'    : 'daily'\n",
    "                ,'start'    : yr.strftime(\"%Y\") + '0101'\n",
    "                ,'end'      : yr.strftime(\"%Y\") + '1231'\n",
    "                ,'stability': 'stable'\n",
    "            }\n",
    "\n",
    "            r_outer = requests.post(\n",
    "                 url\n",
    "                ,data = payload_outer\n",
    "                ,headers = headers\n",
    "            );\n",
    "            resp_outer = r_outer.json();\n",
    "\n",
    "            result_url = None;\n",
    "            for waiting in range(35):\n",
    "                payload_inner = {\n",
    "                     'call' :'pp/checkup'\n",
    "                    ,'proc' : 'gridserv'\n",
    "                    ,'gricket': resp_outer['gricket']\n",
    "                }\n",
    "\n",
    "                r_inner = requests.post(\n",
    "                     url\n",
    "                    ,data = payload_inner\n",
    "                    ,headers = headers\n",
    "                );\n",
    "                resp_inner = r_inner.json();\n",
    "\n",
    "                if 'delay' in resp_inner:\n",
    "                    #print(resp_inner['delay']['status']);\n",
    "                    pass\n",
    "                elif 'result' in resp_inner:\n",
    "                    result_url = resp_inner['result']['csv'];\n",
    "                    break;\n",
    "\n",
    "                sleep(sleeper);\n",
    "\n",
    "            if result_url is None:\n",
    "                raise Exception('never got back results for ' + str(k) + ' for ' + yr.strftime(\"%Y\"));\n",
    "\n",
    "            with open(target_file,'ab') as f:\n",
    "\n",
    "                with requests.get(\n",
    "                     pickup + result_url\n",
    "                    ,stream = True\n",
    "                ) as r:\n",
    "                    skip = 1;\n",
    "                    for line in r.iter_lines():\n",
    "                        if skip > 11 and len(line) > 0:\n",
    "                            f.write(line+'\\n'.encode());\n",
    "                        skip += 1;\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir   = os.getcwd() + os.sep + 'prism';\n",
    "target_file  = target_dir  + os.sep + 'prism_20210816.csv';\n",
    "\n",
    "if os.path.exists(target_file):\n",
    "    os.remove(target_file);\n",
    "    \n",
    "with open(target_file,'w') as outfile:\n",
    "\n",
    "for k,v in tickers.items():\n",
    "    \n",
    "    source_file = target_dir + os.sep + 'batch' + str(k) + '.csv';\n",
    "    \n",
    "    with open(source_file,'r') as infile:\n",
    "\n",
    "        for line in infile:\n",
    "            outfile.write(line);\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
